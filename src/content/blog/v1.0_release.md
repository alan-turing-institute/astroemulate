---
title: "AutoEmulate v1.0.0 release"
pubDate: "2025-07-31"
description: "Updates on new features in v1.0.0 release"
draft: true
author: "AutoEmulate contributors"
release: "v1.0.0"
---

ğŸ‰ We're excited to announce AutoEmulate's first major release with v1.0.0 ğŸ‰

<br/><br/>
This release sees the package fully integrated into the PyTorch ecosystem; PyTorch is one of the most popular open-source deep learning frameworks and one of the bedrocks of modern machine learning with Python.

This release also contains new features, including methods for uncertainty quantification;  ensemble-based emulator models, which output predictions with uncertainty, and Bayesian model calibration, which enables inference of parameters likely to have generated observed data with uncertainty.

<br/><br/>
If you'd like to discuss any of our work on AutoEmulate or AI for Physical Systems, feel free to reach out to us at ai4physics@turing.ac.uk

## What's new

### PyTorch refactor and package redesign

AutoEmulate was originally built on top of the scikit-learn framework, which is a great library for traditional machine learning tasks. However, as we moved towards more complex models and larger datasets, we found that we needed the flexibility and performance that PyTorch provides.

<br/><br/>
The new PyTorch backend allows AutoEmulate to take advantage of PyTorch's powerful features, including (but not limited to):

- Leveraging GPU acceleration, making training and inference much faster.
- Enabling automatic differentiation via PyTorch's autograd system.
- Seamlessly integrating AutoEmulate with other tools in the broader ecosystem, enabling end-to-end emulation workflows.

### A new set of Emulator models

AutoEmulate v1.0.0 includes a new set of Emulator models with a variety of different architectures, built on top of PyTorch (and GPyTorch for Gaussian processes). Many of these are custom implementations built by the AutoEmulate team, including ensemble models where we quantify uncertainty by looking at the distribution over outputs, but some are simply wrapped PyTorch functions.
<br/>
Additionally, we have retained support for several non-PyTorch models that our usersbase has found useful. The table below summarises which emulators available in this release are implemented in PyTorch, support multi-output emulation (MO), use auto-differentiation (AD) for training, or support uncertainty quantification (UQ).

| Emulator | PyTorch | MO | AD | UQ |
|----------|-------|---------|----|----|
| GaussianProcessExact | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
| GaussianProcessExactCorrelated | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
| EnsembleMLP | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
| EnsembleMLPDropout | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
| MLP | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âŒ |
| PolynomialRegression | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âŒ |
| RadialBasisFunctions | âœ”ï¸ | âœ”ï¸ | âŒ | âŒ |
| RandomForest | scikit-learn  | âœ”ï¸ | âŒ | âŒ |
| SupportVectorMachine | scikit-learn | âŒ | âŒ | âŒ |
| LightGBM | LightGBM | âŒ | âŒ | âŒ |

### Bayesian model calibration

Bayesian calibration is a method for estimating which input parameters were most likely to produce observed data. An advantage over other calibration methods is that it returns a probability distribution over the input parameters rather than just point estimates.
Performing Bayesian calibration requires a fit emulator and observations associated with the simulator output. Check out our new [tutorial](<URL>) for more details on how to use this feature.

### Upgraded features

- **Simulator in the loop**: Some features in AutoEmulate such as *active learning* and *history matching* require the user to provide a simulator that can be run by AutoEmulate. AutoEmulate now supports easy integration of simulators into the emulation workflow. This is achieved through a subclassing mechanism of the *Simulator* base class, allowing the users to define a method to run their simulation, taking in the input parameters and returning the output variables. See our [custom simulations tutorial](<URL>) for more details.
- **Active learning** is now a core feature of AutoEmulate. Active learning intelligently selects informative simulator evaluations to maximize emulator improvement with minimal computational cost. See our [active learning tutorial](<URL>) for more details.
- **History matching** *TODO: explain changes*. See our two updated history matching tutorials [1](<URL>) and [2](<URL>) for more details.

### Key API changes

As with previous releases, AutoEmulate v1.0.0 can be used to quickly get an emulator from simulation inputs (x) and outputs (y). This now happens in a single step:

```python
ae = AutoEmulate(x, y)
```

This replaces the previous two-step process of `ae.setup(x, y)` and `ae.compare()`. Check out the [Quick Start Guide](<URL>) for more details on how to use the new API.

## What's next

Looking ahead, we are working on providing support for more complex simulation outputs including spatial and temporal data, which will allow a wider range of applications to benefit from AutoEmulate.
